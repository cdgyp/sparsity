{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _SparseJumpingSquaredReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        x = x.float()\n",
    "        nonzeros = (x > 0).to_sparse_csr()\n",
    "        nonzero_x_shifted = torch.masked.masked_tensor(x * nonzeros, nonzeros).add_(1)\n",
    "        ctx.save_for_backward(nonzero_x_shifted)\n",
    "\n",
    "        masked_jsrelu = nonzero_x_shifted.clone().square_().add_(-1).div_(2)\n",
    "\n",
    "        res: torch.Tensor =  masked_jsrelu.get_data()\n",
    "        assert res.is_sparse or res.is_sparse_csr, res\n",
    "\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        nonzero_x_shifted, = ctx.saved_tensors\n",
    "        return grad_output * nonzero_x_shifted.get_data()\n",
    "    \n",
    "SparseJumpingSquaredReLU = _SparseJumpingSquaredReLU.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn([128, 128], device='cuda', requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y = SparseJumpingSquaredReLU(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.wi = nn.Linear(128, 128)\n",
    "        self.wo = nn.Linear(128, 128)\n",
    "    def forward(self, x):\n",
    "        x = self.wi(x)\n",
    "        x = SparseJumpingSquaredReLU(x)\n",
    "        x = self.wo(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2390, -0.0738, -0.4273,  ...,  0.0761, -0.2038,  0.1887],\n",
       "        [ 0.0366, -0.3345,  0.0172,  ...,  0.4337,  0.2052, -0.2865],\n",
       "        [ 0.1740,  0.0359, -0.2689,  ...,  0.0223, -0.4306,  0.1324],\n",
       "        ...,\n",
       "        [ 0.5264, -0.1293, -0.2503,  ...,  0.1437, -0.2875, -0.2538],\n",
       "        [ 0.1357,  0.1196, -0.0615,  ...,  0.2817,  0.3430, -0.2425],\n",
       "        [ 0.3831,  0.3054,  0.1942,  ...,  0.2678,  0.4195,  0.4509]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLP().to('cuda')\n",
    "with torch.no_grad():\n",
    "    z = mlp(x)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1903,  0.1982,  0.6741,  ..., -0.2116,  0.2345,  0.3611],\n",
      "        [-0.3962,  0.6045, -0.5680,  ...,  0.3701, -1.4278, -0.6893],\n",
      "        [ 0.1720,  0.1340, -0.0762,  ..., -0.1806, -0.4827,  0.0148],\n",
      "        ...,\n",
      "        [-0.3116,  0.0321, -0.2780,  ...,  0.3731, -0.6803, -0.3226],\n",
      "        [ 0.2022, -0.0880,  0.1228,  ...,  0.1000, -0.1772, -0.2030],\n",
      "        [ 0.0146,  0.6470,  0.2027,  ..., -0.4298, -0.1915,  0.6884]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function _SparseJumpingSquaredReLUBackward returned an invalid gradient at index 0 - expected layout Strided but got SparseCsr",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/pz/sparsity/codes/tests/sparse.ipynb 单元格 7\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c313033227d/home/pz/sparsity/codes/tests/sparse.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m z \u001b[39m=\u001b[39m mlp(x)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c313033227d/home/pz/sparsity/codes/tests/sparse.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m loss \u001b[39m=\u001b[39m (z\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39msum()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224d4c313033227d/home/pz/sparsity/codes/tests/sparse.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n",
      "File \u001b[0;32m/data/pz/.conda/envs/pzbase/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/data/pz/.conda/envs/pzbase/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function _SparseJumpingSquaredReLUBackward returned an invalid gradient at index 0 - expected layout Strided but got SparseCsr"
     ]
    }
   ],
   "source": [
    "x.requires_grad = True\n",
    "\n",
    "z = mlp(x)\n",
    "loss = (z**2).sum()\n",
    "loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pzbase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
