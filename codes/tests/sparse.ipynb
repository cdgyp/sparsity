{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.masked import MaskedTensor\n",
    "from torch.masked.maskedtensor.core import _tensors_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.overrides import get_default_nowrap_functions\n",
    "def _validate_members(self):\n",
    "    data = self._masked_data if self._masked_data.is_sparse else self._masked_data\n",
    "    mask = self.get_mask()\n",
    "    if type(data) != type(mask):\n",
    "        raise TypeError(f\"data and mask must have the same type. Got {type(data)} and {type(mask)}\")\n",
    "    if data.layout not in {torch.strided, torch.sparse_coo, torch.sparse_csr}:\n",
    "        raise TypeError(f\"data layout of {data.layout} is not supported.\")\n",
    "    if data.layout == torch.sparse_coo:\n",
    "        if not _tensors_match(data.indices(), mask.indices(), exact=True):\n",
    "            raise ValueError(\"data and mask are both sparse COO tensors but do not have the same indices.\")\n",
    "    elif data.layout == torch.sparse_csr:\n",
    "        if not _tensors_match(\n",
    "            data.crow_indices(), mask.crow_indices(), exact=True\n",
    "        ) or not _tensors_match(data.col_indices(), mask.col_indices(), exact=True):\n",
    "            raise ValueError(\"data and mask are both sparse CSR tensors but do not share either crow or col indices.\")\n",
    "    if mask.dtype != torch.bool:\n",
    "        raise TypeError(\"mask must have dtype bool.\")\n",
    "    if not (\n",
    "        data.dtype == torch.float16\n",
    "        or data.dtype == torch.float32\n",
    "        or data.dtype == torch.float64\n",
    "        or data.dtype == torch.bool\n",
    "        or data.dtype == torch.int8\n",
    "        or data.dtype == torch.int16\n",
    "        or data.dtype == torch.int32\n",
    "        or data.dtype == torch.int64\n",
    "    ):\n",
    "        raise TypeError(f\"{data.dtype} is not supported in MaskedTensor.\")\n",
    "    if data.dim() != mask.dim():\n",
    "        raise ValueError(\"data.dim() must equal mask.dim()\")\n",
    "    if data.size() != mask.size():\n",
    "        raise ValueError(\"data.size() must equal mask.size()\")\n",
    "\n",
    "def _set_data_mask(self, data, mask):\n",
    "    self._masked_data = data.coalesce() if data.is_sparse else data\n",
    "    self._masked_mask = mask\n",
    "    self._validate_members()\n",
    "\n",
    "_old_preprocess_data = MaskedTensor._preprocess_data\n",
    "\n",
    "def _preprocess_data(self, data, mask):\n",
    "    _old_preprocess_data(self, data, mask)\n",
    "    if self._masked_data.is_sparse:\n",
    "        self._masked_data.coalesce()\n",
    "\n",
    "MaskedTensor._validate_members = _validate_members\n",
    "MaskedTensor._set_data_mask = _set_data_mask\n",
    "MaskedTensor._preprocess_data = _preprocess_data\n",
    "\n",
    "    # @classmethod\n",
    "    # def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
    "        # print(func)\n",
    "        # return MaskedTensor.__torch_function__(func, types, args, kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _SparseCOOJumpingSquaredReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x: torch.Tensor):\n",
    "        ctx.x_shape = x.shape\n",
    "        x = x.float().flatten(0, 1)\n",
    "        nonzeros = (x > 0).to_sparse_coo()\n",
    "        nonzero_x_shifted = MaskedTensor(x * nonzeros, nonzeros).add_(1)\n",
    "        ctx.save_for_backward(nonzero_x_shifted)\n",
    "\n",
    "        masked_jsrelu = nonzero_x_shifted.clone().square_().add_(-1).div_(2)\n",
    "\n",
    "        res: torch.Tensor =  masked_jsrelu.get_data()\n",
    "        assert res.is_sparse or res.is_sparse_csr, res\n",
    "\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        nonzero_x_shifted, = ctx.saved_tensors\n",
    "        return (grad_output * nonzero_x_shifted.get_data()).to_dense().reshape(ctx.x_shape)\n",
    "    \n",
    "SparseCOOJumpingSquaredReLU = _SparseCOOJumpingSquaredReLU.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class SparseCOOLinear(nn.Linear):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True, device=None, dtype=None, n_token=None) -> None:\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "        self.n_token = n_token\n",
    "    def forward(self, input: Tensor, n_token=None) -> Tensor:\n",
    "        if not input.is_sparse and not input.is_sparse_csr:\n",
    "            return super().forward(input)\n",
    "        assert len(input.shape) <= 2\n",
    "        output: torch.Tensor = torch.sparse.addmm(self.bias, input, self.weight.transpose(-1, -2))\n",
    "        if n_token is None:\n",
    "            n_token = self.n_token\n",
    "        return output.unflatten(0, [-1, n_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseCOOMLP(nn.Module):\n",
    "    def __init__(self, dim=768, hidden_dim=3072, n_token=None) -> None:\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(dim, hidden_dim)\n",
    "        self.act = SparseCOOJumpingSquaredReLU\n",
    "        self.value = SparseCOOLinear(hidden_dim, dim, n_token=n_token)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        activation = self.act(self.key(x))\n",
    "        return self.value(activation, n_token=x.shape[1])\n",
    "        \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim=768, hidden_dim=3072) -> None:\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(dim, hidden_dim)\n",
    "        self.act = nn.ReLU()\n",
    "        self.value = nn.Linear(hidden_dim, dim)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        activation = self.act(self.key(x))\n",
    "        return self.value(activation)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn([64, 197, 768], device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/envs/sharedEnv/lib/python3.8/site-packages/torch/masked/maskedtensor/core.py:156: UserWarning: The PyTorch API of MaskedTensors is in prototype stage and will change in the near future. Please open a Github issue for features requests and see our documentation on the torch.masked module for further information about the project.\n",
      "  warnings.warn((\"The PyTorch API of MaskedTensors is in prototype stage \"\n"
     ]
    }
   ],
   "source": [
    "smlp = SparseCOOMLP().to('cuda')\n",
    "for i in range(10):\n",
    "    y = smlp(x)\n",
    "    loss = (y**2).sum()\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP().to('cuda')\n",
    "for i in range(100):\n",
    "    y_non_sparse = mlp(x)\n",
    "    loss = (y_non_sparse**2).sum()\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class _SparseCSRJumpingSquaredReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x: torch.Tensor):\n",
    "        ctx.x_shape = x.shape\n",
    "        x = x.float().flatten(0, 1)\n",
    "        nonzeros = (x > 0).to_sparse_csr()\n",
    "        nonzero_x_shifted = MaskedTensor(x * nonzeros, nonzeros).add_(1)\n",
    "        ctx.save_for_backward(nonzero_x_shifted)\n",
    "\n",
    "        masked_jsrelu = nonzero_x_shifted.clone().square_().add_(-1).div_(2)\n",
    "\n",
    "        res: torch.Tensor =  masked_jsrelu.get_data()\n",
    "        assert res.is_sparse or res.is_sparse_csr, res\n",
    "\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        nonzero_x_shifted, = ctx.saved_tensors\n",
    "        return (grad_output * nonzero_x_shifted.get_data()).to_dense().reshape(ctx.x_shape)\n",
    "    \n",
    "SparseCSRJumpingSquaredReLU = _SparseCSRJumpingSquaredReLU.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseCSRLinear(nn.Linear):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True, device=None, dtype=None, n_token=None) -> None:\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "        self.n_token = n_token\n",
    "    def forward(self, input: Tensor, n_token=None) -> Tensor:\n",
    "        if not input.is_sparse and not input.is_sparse_csr:\n",
    "            return super().forward(input)\n",
    "        assert len(input.shape) <= 2\n",
    "        output: torch.Tensor = torch.sparse.addmm(self.bias, input, self.weight.transpose(-1, -2))\n",
    "        if n_token is None:\n",
    "            n_token = self.n_token\n",
    "        return output.unflatten(0, [-1, n_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseCSRMLP(nn.Module):\n",
    "    def __init__(self, dim=768, hidden_dim=3072, n_token=None) -> None:\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(dim, hidden_dim)\n",
    "        self.act = SparseCSRJumpingSquaredReLU\n",
    "        self.value = SparseCSRLinear(hidden_dim, dim, n_token=n_token)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        activation = self.act(self.key(x))\n",
    "        return self.value(activation, n_token=x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "csr_mlp = SparseCSRMLP().to('cuda')\n",
    "for i in range(10):\n",
    "    y = csr_mlp(x)\n",
    "    loss = (y**2).sum()\n",
    "    loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pzbase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
